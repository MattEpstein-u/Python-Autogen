{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3359682",
   "metadata": {},
   "source": [
    "# Travel Planning Agents with Google Gemini API\n",
    "\n",
    "This notebook demonstrates multi-agent collaboration using AutoGen with Google's Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3493ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74ae0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google Gemini API client\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "# You can get an API key from: https://makersuite.google.com/app/apikey\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),  # use GOOGLE_API_KEY env var\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb9c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the travel planning agents\n",
    "\n",
    "planner_agent = AssistantAgent(\n",
    "    \"planner_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can plan trips.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request.\",\n",
    ")\n",
    "\n",
    "local_agent = AssistantAgent(\n",
    "    \"local_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A local assistant that can suggest local activities or places to visit.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.\",\n",
    ")\n",
    "\n",
    "language_agent = AssistantAgent(\n",
    "    \"language_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can provide language tips for a given destination.\",\n",
    "    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.\",\n",
    ")\n",
    "\n",
    "travel_summary_agent = AssistantAgent(\n",
    "    \"travel_summary_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can summarize the travel plan.\",\n",
    "    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c6d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a termination condition\n",
    "termination = TextMentionTermination(\"TERMINATE\")\n",
    "\n",
    "# Create the team with RoundRobin group chat\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=[planner_agent, local_agent, language_agent, travel_summary_agent],\n",
    "    termination_condition=termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5392bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Plan a 5-day trip to Mexico City. Include must-see attractions, local experiences, and language tips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for planner_agent_1a080af5-1cde-492b-92aa-1855cdbf2145/1a080af5-1cde-492b-92aa-1855cdbf2145\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1109, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 704, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2678, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1797, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1597, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-pro is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\n",
      "Error processing publish message for local_agent_1a080af5-1cde-492b-92aa-1855cdbf2145/1a080af5-1cde-492b-92aa-1855cdbf2145\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for language_agent_1a080af5-1cde-492b-92aa-1855cdbf2145/1a080af5-1cde-492b-92aa-1855cdbf2145\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for travel_summary_agent_1a080af5-1cde-492b-92aa-1855cdbf2145/1a080af5-1cde-492b-92aa-1855cdbf2145\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-pro is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1109, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 704, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2678, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1797, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1597, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-pro is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_travel_planning()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrun_travel_planning\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_travel_planning\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Console(\n\u001b[32m      6\u001b[39m         team.run_stream(\n\u001b[32m      7\u001b[39m             task=\u001b[33m\"\u001b[39m\u001b[33mPlan a 5-day trip to Mexico City. Include must-see attractions, local experiences, and language tips.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m         )\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-pro is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1109, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 704, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2678, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1797, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/workspaces/Python-Autogen/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1597, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-1.5-pro is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\n"
     ]
    }
   ],
   "source": [
    "# Run the team with a travel planning task\n",
    "import asyncio\n",
    "\n",
    "async def run_travel_planning():\n",
    "    result = await Console(\n",
    "        team.run_stream(\n",
    "            task=\"Plan a 5-day trip to Mexico City. Include must-see attractions, local experiences, and language tips.\"\n",
    "        )\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Run the async function\n",
    "await run_travel_planning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
